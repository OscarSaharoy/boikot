# ollama
## Date: Sun 14 Jan 14:59:36 GMT 2024
## Author: oscar@boikot.xyz

After searching for a good LLM api and even begging anthropic for access to theirs I found ollama and am very happy!! it's open source, it can run lots of different models locally and is super fast if you have a device that works well with it. I am working on a webpage article extractor now and hoping to start summarising and combining articles asap.

ollama: https://ollama.ai/

